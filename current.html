<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Deep Learning Research</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Features</div>
<div class="menu-item"><a href="index.html" class="current">About</a></div>
<div class="menu-item"><a href="past.html">Physics&nbsp;Research</a></div>
<div class="menu-item"><a href="current.html">DL&nbsp;Research</a></div>
<div class="menu-item"><a href="CVpage.html">CV</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Deep Learning Research</h1>
</div>
<p>My work thus far has involved using tools from random matrix theory (free probability) within deep learning as well as Bayesian approaches to deep learning using Gaussian processes.</p>
<h2>Publications</h2>
<p>1. Jeffrey Pennington and <b>Yasaman Bahri</b>. &ldquo;Geometry of Neural Network Loss Surfaces via Random Matrix Theory.&rdquo; ICML, 2017. <a href="http://proceedings.mlr.press/v70/pennington17a">http://proceedings.mlr.press/v70/pennington17a</a></p>
<p>Abstract: Understanding the geometry of neural network loss surfaces is important for the development of improved optimization algorithms and for building a theoretical understanding of why deep learning works. In this paper, we study the geometry in terms of the distribution of eigenvalues of the Hessian matrix at critical points of varying energy. We introduce an analytical framework and a set of tools from random matrix theory that allow us to compute an approximation of this distribution under a set of simplifying assumptions. The shape of the spectrum depends strongly on the energy and another key parameter, phi, which measures the ratio of parameters to data points. Our analysis predicts and numerical simulations support that for critical points of small index, the number of negative eigenvalues scales like the 3/2 power of the energy. We leave as an open problem an explanation for our observation that, in the context of a certain memorization task, the energy of minimizers is well-approximated by the function 1/2(1-phi)^2.</p>
<p>2. Jaehoon Lee*, <b>Yasaman Bahri</b>*, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein. &ldquo;Deep Neural Networks as Gaussian Processes.&rdquo; <a href="https://arxiv.org/abs/1711.00165">arxiv:1711.00165</a>. Submitted, 2017.<br />
*<i>Equal contribution</i></p>
<p>Abstract: A deep fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP) in the limit of infinite network width. This correspondence enables exact Bayesian inference for neural networks on regression tasks by means of straightforward matrix computations. For single hidden-layer networks, the covariance function of this GP has long been known. Recently, kernel functions for multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified the correspondence between using these kernels as the covariance function for a GP and performing fully Bayesian prediction with a deep neural network. In this work, we derive this correspondence and develop a computationally efficient pipeline to compute the covariance functions. We then use the resulting GP to perform Bayesian inference for deep neural networks on MNIST and CIFAR-10. We find that the GP-based predictions are competitive and can outperform neural networks trained with stochastic gradient descent. We observe that the trained neural network accuracy approaches that of the corresponding GP-based computation with increasing layer width, and that the GP uncertainty is strongly correlated with prediction error. We connect our observations to the recent development of signal propagation in random neural networks.</p>
<div id="footer">
<div id="footer-text">
Page generated 2017-12-02 17:00:26 PST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
(<a href="current.jemdoc">source</a>)
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
