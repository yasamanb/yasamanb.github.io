<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Deep Learning Research</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Features</div>
<div class="menu-item"><a href="index.html" class="current">About</a></div>
<div class="menu-item"><a href="current.html">Deep&nbsp;Learning&nbsp;Research</a></div>
<div class="menu-item"><a href="past.html">Physics&nbsp;Research</a></div>
<div class="menu-item"><a href="CVpage.html">Curriculum&nbsp;Vitae</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Deep Learning Research</h1>
</div>
<p>My research program thus far has been to systematically build out a unified understanding of supervised learning, theoretically and empirically. A fruitful approach underlying some (but not all) of my recent work has been to investigate deep networks that are wide (have many hidden units per layer). More broadly, some concrete results have been:</p>
<p><br /></p>
<ul>
<li><p>Establishing the nature of function-space priors in deep neural networks of various architectures (connection to Gaussian Processes). This enables exact Bayesian inference in infinitely-wide networks. These connections initiated a flurry of subsequent work by the community on infinitely-wide deep networks. (2, 5, 9)</p>
</li>
</ul>
<ul>
<li><p>Concrete initialization schemes for training deep networks, based on a recursive analytic characterization of deep network priors. (4)</p>
</li>
</ul>
<ul>
<li><p>Developing models for the Hessian of deep networks that incorporates their  structure, moving beyond simple models from random matrix theory. Introduced new tools from random matrix theory in deep learning. (1)</p>
</li>
</ul>
<ul>
<li><p>Establishing the connection between SGD-trained infinitely-wide networks and linear models. (6)</p>
</li>
</ul>
<ul>
<li><p>Empirical investigation of particular measures and their connection to good generalization in deep networks. (3)</p>
</li>
</ul>
<ul>
<li><p>The existence of distinct <i> phases </i> in wide, deep networks as a function of learning rate (in SGD), a phenomenon we find to be rather universal. The large learning rate phase involves significant kernel learning and is a  regime of interest for practice. We present this newly identified phenomenon, the implications for generalization, and provide an original theoretical approach towards describing it. The phenomenon is one of a few examples of a strict phase transition in deep learning. (8)</p>
</li>
</ul>
<h2>Conference &amp; Journal Publications</h2>
<p>1. Jeffrey Pennington and <b>Yasaman Bahri</b>. &ldquo;Geometry of Neural Network Loss Surfaces via Random Matrix Theory.&rdquo; ICML 2017. <a href="http://proceedings.mlr.press/v70/pennington17a">http://proceedings.mlr.press/v70/pennington17a</a></p>
<p>2. Jaehoon Lee*, <b>Yasaman Bahri</b>*, Roman Novak, Sam S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein. &ldquo;Deep Neural Networks as Gaussian Processes.&rdquo; ICLR 2018. <a href="https://arxiv.org/abs/1711.00165">https://arxiv.org/abs/1711.00165</a>. <br />
*<i>Equal contribution</i></p>
<p>3. Roman Novak, <b>Yasaman Bahri</b>, Dan Abolafia, Jeffrey Pennington, Jascha Sohl-Dickstein. &ldquo;Sensitivity and Generalization in Neural Networks: an Empirical Study.&rdquo; ICLR 2018. <a href="https://arxiv.org/abs/1802.08760">https://arxiv.org/abs/1802.08760</a>. <br /></p>
<p>4. Lechao Xiao, <b>Yasaman Bahri</b>, Jascha Sohl-Dickstein, Sam S. Schoenholz, Jeffrey Pennington. &ldquo;Dynamical isometry and a mean field theory of convolutional networks: how to train 10,000-layer vanilla CNNs.&rdquo; ICML 2018. <a href="https://arxiv.org/abs/1806.05393">https://arxiv.org/abs/1806.05393</a>. </p>
<p>5. Roman Novak, Lechao Xiao, Jaehoon Lee*, <b>Yasaman Bahri</b>*, Greg Yang, Jiri Hron, Dan Abolafia, Jeffrey Pennington, Jascha Sohl-Dickstein. &ldquo;Bayesian Convolutional Neural Networks with Many Channels are Gaussian Processes.&rdquo; ICLR 2019. <a href="https://arxiv.org/abs/1810.05148">https://arxiv.org/abs/1810.05148</a>. *<i>Equal contribution</i></p>
<p>6. Jaehoon Lee*, Lechao Xiao*, Sam S. Schoenholz, <b>Yasaman Bahri</b>, Jascha Sohl-Dickstein, Jeffrey Pennington. &ldquo;Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent.&rdquo; NeurIPS 2019. <a href="https://arxiv.org/abs/1902.06720">https://arxiv.org/abs/1902.06720</a>. <br /> </p>
<p>Also published in Journal of Statistical Mechanics: Theory and Experiment (2020) 124002.</p>
<p>7. <b>Yasaman Bahri</b>, Jonathan Kadmon, Jeffrey Pennington, Sam S. Schoenholz, Jascha Sohl-Dickstein, Surya Ganguli. &ldquo;Statistical Mechanics of Deep Learning.&rdquo; Annual Review of Cond. Matter Physics, 2020.</p>
<p>8. Aitor Lewkowycz, <b>Yasaman Bahri</b>, Ethan Dyer, Jascha Sohl-Dickstein, Guy Gur-Ari. &ldquo;The large learning rate phase of deep learning: the catapult mechanism.&rdquo; Submitted. <a href="https://arxiv.org/abs/2003.02218">https://arxiv.org/abs/2003.02218</a>. </p>
<p>9. Jiri Hron, <b>Yasaman Bahri</b>, Jascha Sohl-Dickstein, Roman Novak. &ldquo;Infinite-attention: NNGP and NTK for deep attention networks.&rdquo; ICML 2020. </p>
<div id="footer">
<div id="footer-text">
Page generated 2021-01-03 14:13:48 PST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
(<a href="current.jemdoc">source</a>)
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
