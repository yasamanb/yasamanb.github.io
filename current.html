<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Deep Learning Research</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Features</div>
<div class="menu-item"><a href="index.html" class="current">About</a></div>
<div class="menu-item"><a href="CVpage.html">Curriculum&nbsp;Vitae</a></div>
<div class="menu-item"><a href="current.html">Deep&nbsp;Learning&nbsp;Research</a></div>
<div class="menu-item"><a href="past.html">Physics&nbsp;Research</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Deep Learning Research</h1>
</div>
<p>My research program thus far has been to systematically build out a unified understanding of supervised learning, theoretically and empirically. A fruitful approach underlying some (but not all) of my recent work has been to investigate deep networks that are wide (have many hidden units per layer). More broadly, some concrete results have been:</p>
<ul>
<li><p>Establishing the nature of function-space priors in deep neural networks of various architectures (connection to Gaussian Processes). This enables exact Bayesian inference in infinitely-wide networks. These connections initiated a flurry of subsequent work by the community on infinitely-wide deep networks. (8, 6, 3)</p>
</li>
</ul>
<ul>
<li><p>Establishing the connection between SGD-trained infinitely-wide networks and linear models. (5)</p>
</li>
</ul>
<ul>
<li><p>The existence of distinct <i> phases </i> in wide, deep networks as a function of learning rate (in SGD), a phenomenon we find to be rather universal. The large learning rate phase involves significant kernel learning and is a  regime of interest for practice. We present this newly identified phenomenon, the implications for generalization, and provide an original theoretical approach towards describing it. The phenomenon is one of a few examples of a strict phase transition in deep learning. (2)</p>
</li>
</ul>
<ul>
<li><p>Theory for understanding the origin of <i>scaling laws</i> in the test loss of neural networks as a function of dataset and model size. Amongst other results, we propose a novel classification framework for these exponents (&ldquo;variance-limited&rdquo; and &ldquo;resolution-limited&rdquo; regimes). (1)</p>
</li>
</ul>
<ul>
<li><p>Concrete initialization schemes for training deep networks, based on a recursive analytic characterization of deep network priors. (7)</p>
</li>
</ul>
<ul>
<li><p>Developing models for the Hessian of deep networks that incorporates their  structure, moving beyond simple models from random matrix theory. Introduced new tools from random matrix theory in deep learning. (10)</p>
</li>
</ul>
<ul>
<li><p>Empirical investigation of particular measures and their connection to good generalization in deep networks. (9)</p>
</li>
</ul>
<h2>Conference &amp; Journal Publications</h2>
<p>[1]. <a href="https://arxiv.org/abs/2102.06701">&ldquo;Explaining Neural Scaling Laws&rdquo;</a> <br />
<b>Yasaman Bahri</b>, Ethan Dyer, Jared Kaplan, Jaehoon Lee, Utkarsh Sharma <br /> 
<i>(Alphabetical order</i>) <br /> 
Under review <br /></p>
<p>[2]. <a href="https://arxiv.org/abs/2003.02218">&ldquo;The large learning rate phase of deep learning: the catapult mechanism&rdquo;</a> <br />
Aitor Lewkowycz, <b>Yasaman Bahri</b>, Ethan Dyer, Jascha Sohl-Dickstein, Guy Gur-Ari <br /> 
In submission <br /></p>
<p>[3]. <a href="http://proceedings.mlr.press/v119/hron20a">&ldquo;Infinite-attention: NNGP and NTK for deep attention networks&rdquo;</a> <br />
Jiri Hron, <b>Yasaman Bahri</b>, Jascha Sohl-Dickstein, Roman Novak  <br />
ICML 2020 (International Conference on Machine Learning)<br /></p>
<p>[4]. <a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-conmatphys-031119-050745">&ldquo;Statistical Mechanics of Deep Learning&rdquo;</a> <br />
<b>Yasaman Bahri</b>, Jonathan Kadmon, Jeffrey Pennington, Sam S. Schoenholz, Jascha Sohl-Dickstein, Surya Ganguli <br />
Annual Review of Condensed Matter Physics (2020) <br /></p>
<p>[5]. <a href="https://papers.nips.cc/paper/2019/hash/0d1a9651497a38d8b1c3871c84528bd4-Abstract.html">&ldquo;Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent&rdquo;</a> <br />
Jaehoon Lee*, Lechao Xiao*, Sam S. Schoenholz, <b>Yasaman Bahri</b>, Jascha Sohl-Dickstein, Jeffrey Pennington <br />
NeurIPS 2019 (Advances in Neural Information Processing Systems) <br />
Also published in Journal of Statistical Mechanics: Theory and Experiment (2020) 124002 <br /></p>
<p>[6]. <a href="https://arxiv.org/abs/1810.05148">&ldquo;Bayesian Convolutional Neural Networks with Many Channels are Gaussian Processes&rdquo;</a> <br />
Roman Novak, Lechao Xiao, Jaehoon Lee*, <b>Yasaman Bahri</b>*, Greg Yang, Jiri Hron, Dan Abolafia, Jeffrey Pennington, Jascha Sohl-Dickstein <br />
*Equal contribution <br />
ICLR 2019 (International Conference on Learning Representations) <br /></p>
<p>[7]. <a href="https://arxiv.org/abs/1806.05393">&ldquo;Dynamical isometry and a mean field theory of convolutional networks: how to train 10,000-layer vanilla CNNs&rdquo;</a> <br />
Lechao Xiao, <b>Yasaman Bahri</b>, Jascha Sohl-Dickstein, Sam S. Schoenholz, Jeffrey Pennington <br />
ICML 2018 (International Conference on Machine Learning) <br /></p>
<p>[8]. <a href="https://arxiv.org/abs/1711.00165">&ldquo;Deep Neural Networks as Gaussian Processes&rdquo;</a> <br />
Jaehoon Lee*, <b>Yasaman Bahri</b>*, Roman Novak, Sam S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein <br />
*<i>Equal contribution</i> <br />
ICLR 2018 (International Conference on Learning Representations) <br /></p>
<p>[9]. <a href="https://arxiv.org/abs/1802.08760">&ldquo;Sensitivity and Generalization in Neural Networks: an Empirical Study&rdquo;</a> <br />
Roman Novak, <b>Yasaman Bahri</b>, Dan Abolafia, Jeffrey Pennington, Jascha Sohl-Dickstein <br />
ICLR 2018 (International Conference on Learning Representations) <br /></p>
<p>[10]. <a href="http://proceedings.mlr.press/v70/pennington17a">&ldquo;Geometry of Neural Network Loss Surfaces via Random Matrix Theory&rdquo;</a> <br />
Jeffrey Pennington and <b>Yasaman Bahri</b> <br />
ICML 2017 (International Conference on Machine Learning)</p>
<div id="footer">
<div id="footer-text">
Page generated 2021-04-05 20:44:33 PDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
(<a href="current.jemdoc">source</a>)
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
