<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Deep Learning Research</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Features</div>
<div class="menu-item"><a href="index.html" class="current">About</a></div>
<div class="menu-item"><a href="past.html">Physics&nbsp;Research</a></div>
<div class="menu-item"><a href="current.html">Deep&nbsp;Learning&nbsp;Research</a></div>
<div class="menu-item"><a href="CVpage.html">CV</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Deep Learning Research</h1>
</div>
<p>My work thus far has focused on supervised learning and various aspects of optimization and generalization. This has included principled initialization schemes for training deeper networks (4);  characterizing properties of networks that generalize well (3); developing models for the loss landscape (Hessian) that incorporate the structure of neural networks, utilizing random matrix theory (1); and constructing exact mappings of infinitely wide fully-connected (2) or convolutional (5) networks, using the mappings to perform exact Bayesian inference, and comparing to gradient-descent based optimization.</p>
<h2>Summary of Publications</h2>
<p>1. Jeffrey Pennington and <b>Yasaman Bahri</b>. &ldquo;Geometry of Neural Network Loss Surfaces via Random Matrix Theory.&rdquo; ICML 2017. <a href="http://proceedings.mlr.press/v70/pennington17a">http://proceedings.mlr.press/v70/pennington17a</a></p>
<p>2. Jaehoon Lee*, <b>Yasaman Bahri</b>*, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein. &ldquo;Deep Neural Networks as Gaussian Processes.&rdquo; ICLR 2018. <a href="https://arxiv.org/abs/1711.00165">https://arxiv.org/abs/1711.00165</a>. <br />
*<i>Equal contribution</i></p>
<p>3. Roman Novak, <b>Yasaman Bahri</b>, Dan Abolafia, Jeffrey Pennington, Jascha Sohl-Dickstein. &ldquo;Sensitivity and Generalization in Neural Networks: an Empirical Study.&rdquo; ICLR 2018. <a href="https://arxiv.org/abs/1802.08760">https://arxiv.org/abs/1802.08760</a>. <br /></p>
<p>4. Lechao Xiao, <b>Yasaman Bahri</b>, Jascha Sohl-Dickstein, Samuel S. Schoenholz, Jeffrey Pennington. &ldquo;Dynamical isometry and a mean field theory of convolutional networks: how to train 10,000-layer vanilla CNNs.&rdquo; <a href="https://arxiv.org/abs/1806.05393">https://arxiv.org/abs/1806.05393</a>. ICML 2018.</p>
<p>5. Roman Novak, Lechao Xiao, Jaehoon Lee*, <b>Yasaman Bahri</b>*, Greg Yang, Jiri Hron, Dan Abolafia, Jeffrey Pennington, Jascha Sohl-Dickstein. &ldquo;Bayesian Convolutional Neural Networks with Many Channels are Gaussian Processes.&rdquo; ICLR 2019. <a href="https://arxiv.org/abs/1810.05148">https://arxiv.org/abs/1810.05148</a>. *<i>Equal contribution</i></p>
<p>6. Jaehoon Lee*, Lechao Xiao*, Sam Schoenholz, <b>Yasaman Bahri</b>, Jascha Sohl-Dickstein, Jeffrey Pennington. &ldquo;Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent.&rdquo; <a href="https://arxiv.org/abs/1902.06720">https://arxiv.org/abs/1902.06720</a>. Submitted, 2019.</p>
<div id="footer">
<div id="footer-text">
Page generated 2019-04-01 22:04:42 PDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
(<a href="current.jemdoc">source</a>)
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
